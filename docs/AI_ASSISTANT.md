# TaskFlow AI Assistant

<details>
<summary>English version</summary>

---

# TaskFlow AI Assistant (English)

## Overview

TaskFlow ships with an embedded AI assistant focused on task management. The backend now targets the [Hugging Face router](https://huggingface.co/inference-api) by default; Ollama remains available only as an offline fallback for edge cases.

## Main Features

- üí¨ Modern chat UI with conversation history
- üîí Strict task-management scope enforced by a shared system prompt
- üéØ Personalized responses that incorporate your tasks, categories, and achievements
- üåê Automatic language detection (Spanish or English)
- üîê JWT-protected API; no conversation persistence
- ‚öôÔ∏è Cloud-first provider (Hugging Face router) with optional Ollama fallback

## Requirements

### Hugging Face (Default)

1. Create or use an existing Hugging Face account.
2. Generate a **Write** access token from https://huggingface.co/settings/tokens.
3. Pick a text-generation model that is available to your token (the router exposes a catalog at `GET https://router.huggingface.co/v1/models`; the defaults below use `HuggingFaceTB/SmolLM3-3B`).
4. (Optional) Upgrade the workspace if you need higher rate limits or private models.

### Fallback: Ollama (Local, optional)

Only configure [Ollama](https://ollama.com) if you need to run completely offline or keep all inference on the workstation (`ollama pull llama3.2:latest`). Switch providers by setting `AI__PROVIDER=ollama` when Hugging Face is not an option.

## Configuration

### Backend (.NET)

The assistant reads strongly-typed options from the `AI` configuration section. Add the following to your environment-specific settings (or populate the corresponding environment variables):

```json
{
	"AI": {
		"Provider": "huggingface",
		"ApiKey": "hf_your_write_token",
		"Model": "HuggingFaceTB/SmolLM3-3B",
		"BaseUrl": "https://router.huggingface.co",
		"TimeoutSeconds": 90
	}
}
```

Environment variable overrides are also supported:

- `AI__PROVIDER`
- `AI__APIKEY` (or legacy `HUGGINGFACE_API_KEY`)
- `AI__MODEL`
- `AI__BASEURL`
- `AI__TIMEOUTSECONDS`

The dependency injection container automatically selects the correct provider:

- `huggingface` ‚Üí `HuggingFaceProvider`
- `ollama` ‚Üí `OllamaProvider` (use only when Hugging Face is unavailable)
- anything else ‚Üí defaults back to Hugging Face

### Frontend (.env)

No changes are required for the frontend, but the status banner surfaces when the provider is offline or misconfigured so users know to verify credentials.

## Usage

1. Log in to TaskFlow.
2. Open the purple floating action button at the bottom-right corner.
3. The header chip shows `Online` when the provider responds to health checks.
4. Ask for task-planning advice, prioritization tips, or category suggestions. The assistant will decline unrelated topics.

## Customization

- **System Prompt**: update `SYSTEM_PROMPT` in `TaskFlow.Api/Services/AIAssistantService.cs`.
- **Prompt Formatting**: adjust `AiPromptBuilder` if you want to change how tasks/categories are injected.
- **Generation Parameters**: tweak `MaxNewTokens`, `Temperature`, and `TopP` inside `TaskFlow.Api/Services/HuggingFaceProvider.cs`.
- **Provider Swap**: keep `AI__PROVIDER=huggingface` for production; switch to `ollama` only for offline demos or emergency fallback.

## Troubleshooting

| Symptom | Checks |
| --- | --- |
| `AI Assistant is not available` banner | Ensure `AI__APIKEY` is set, the selected model exists, and your Hugging Face account has quota. If you intentionally switched to Ollama, confirm the daemon is running and the model is downloaded. |
| 401 Unauthorized responses | Regenerate the Hugging Face token and make sure it has **Write** permissions. |
| 429 Too Many Requests | Upgrade your Hugging Face plan or slow down request frequency. |
| Slow first response | Hugging Face may need to spin up the model; retry after a few seconds. For Ollama, the model loads into memory the first time. |

## Security & Privacy

- No conversation data is persisted in the database.
- Requests are only sent to the configured provider (Hugging Face router by default, or Ollama when explicitly selected).
- API access requires a valid JWT.
- Only curated task/category summaries are shared with the provider.

## References

- [Hugging Face Inference API](https://huggingface.co/inference-api)
- [Ollama Documentation](https://github.com/ollama/ollama)

</details>

---

# TaskFlow AI Assistant (Espa√±ol)

## Descripci√≥n

TaskFlow incluye un asistente de IA enfocado en la gesti√≥n de tareas. El backend ahora apunta al router de [Hugging Face](https://huggingface.co/inference-api) como opci√≥n principal; Ollama queda reservado como alternativa local para escenarios sin conexi√≥n.

## Caracter√≠sticas Principales

- üí¨ Interfaz de chat moderna con historial
- üîí √Åmbito restringido a temas de productividad y organizaci√≥n
- üéØ Respuestas personalizadas con tus tareas, categor√≠as y logros
- üåê Detecci√≥n autom√°tica de idioma (espa√±ol o ingl√©s)
- üîê API protegida con JWT; no se guardan las conversaciones
- ‚öôÔ∏è Proveedor principal en la nube (router de Hugging Face) con fallback opcional en Ollama

## Requisitos

### Hugging Face (Modo predeterminado)

1. Crea o usa una cuenta existente en Hugging Face.
2. Genera un token con permiso **Write** desde https://huggingface.co/settings/tokens.
3. Eleg√≠ un modelo de texto disponible para tu token (consult√° `GET https://router.huggingface.co/v1/models`; el valor por defecto usa `HuggingFaceTB/SmolLM3-3B`).
4. (Opcional) Mejora tu plan si necesitas m√°s capacidad o modelos privados.

### Fallback: Ollama (local, opcional)

Configur√° [Ollama](https://ollama.com) solo si necesit√°s ejecutar completamente sin conexi√≥n o mantener la inferencia en tu equipo (`ollama pull llama3.2:latest`). Cambi√° el proveedor con `AI__PROVIDER=ollama` √∫nicamente cuando Hugging Face no sea viable.

## Configuraci√≥n

### Backend (.NET)

El asistente lee las opciones desde la secci√≥n `AI` de configuraci√≥n. Agrega lo siguiente a tus settings o variables de entorno:

```json
{
	"AI": {
		"Provider": "huggingface",
		"ApiKey": "hf_tu_token",
		"Model": "HuggingFaceTB/SmolLM3-3B",
		"BaseUrl": "https://router.huggingface.co",
		"TimeoutSeconds": 90
	}
}
```

Variables de entorno soportadas:

- `AI__PROVIDER`
- `AI__APIKEY` (o el legado `HUGGINGFACE_API_KEY`)
- `AI__MODEL`
- `AI__BASEURL`
- `AI__TIMEOUTSECONDS`

El contenedor de dependencias elige el proveedor autom√°ticamente:

- `huggingface` ‚Üí `HuggingFaceProvider`
- `ollama` ‚Üí `OllamaProvider` (usar solo cuando Hugging Face no est√© disponible)
- cualquier otro valor ‚Üí vuelve a Hugging Face por omisi√≥n

### Frontend (.env)

No se requieren cambios. El indicador de estado mostrar√° cuando el proveedor no est√© disponible para que los usuarios revisen las credenciales.

## Uso

1. Inicia sesi√≥n en TaskFlow.
2. Abre el bot√≥n flotante morado en la esquina inferior derecha.
3. El chip en el encabezado mostrar√° `En l√≠nea` cuando el proveedor responda correctamente.
4. Pide sugerencias sobre tus tareas, organizaci√≥n del d√≠a o prioridades. El asistente rechazar√° temas ajenos.

## Personalizaci√≥n

- **System Prompt**: modifica `SYSTEM_PROMPT` en `TaskFlow.Api/Services/AIAssistantService.cs`.
- **Formato del contexto**: ajusta `AiPromptBuilder` para cambiar c√≥mo se env√≠an las tareas/categor√≠as.
- **Par√°metros de generaci√≥n**: modifica `MaxNewTokens`, `Temperature` y `TopP` en `TaskFlow.Api/Services/HuggingFaceProvider.cs`.
- **Cambiar proveedor**: manten√© `AI__PROVIDER=huggingface` en entornos normales; eleg√≠ `ollama` solo para demos offline o contingencia.

## Resoluci√≥n de Problemas

| S√≠ntoma | Verificaciones |
| --- | --- |
| Mensaje "El asistente de IA no est√° disponible" | Aseg√∫rate de definir `AI__APIKEY`, que el modelo exista y que tu cuenta tenga cuota. Si decidiste usar Ollama, confirm√° que el servicio est√© activo y que el modelo se descarg√≥. |
| Respuesta 401 Unauthorized | Regenera el token de Hugging Face y comprueba que tenga permisos **Write**. |
| Respuesta 429 Too Many Requests | Incrementa tu plan en Hugging Face o reduce la frecuencia de llamadas. |
| Primera respuesta lenta | Hugging Face puede demorar en activar el modelo; vuelve a intentar. En Ollama, el modelo se carga en memoria la primera vez. |

## Seguridad y Privacidad

- No se persisten conversaciones en la base de datos.
- Las solicitudes solo se env√≠an al proveedor configurado (router de Hugging Face por defecto u Ollama si lo seleccion√°s expl√≠citamente).
- El acceso requiere un JWT v√°lido.
- Solo se comparten res√∫menes de tareas y categor√≠as necesarios para el contexto.

## Referencias

- [Hugging Face Inference API](https://huggingface.co/inference-api)
- [Documentaci√≥n de Ollama](https://github.com/ollama/ollama)
